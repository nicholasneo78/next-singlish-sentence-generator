{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Phase5-POS-insertion-Singlish-words.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOomCR9HKvHgO56710a186S"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"-qtmIF42v0hB"},"source":["# Phase 5 - POS tagging and insertion of Singlish words"]},{"cell_type":"code","metadata":{"id":"NUsy_OG7xFwE","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1605975343777,"user_tz":-480,"elapsed":17777,"user":{"displayName":"Nicholas Neo","photoUrl":"","userId":"02359484661843204503"}},"outputId":"d16b3a11-63c9-43c3-ca49-689f975e9f34"},"source":["from google.colab import drive\n","drive.mount(\"/content/drive\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"RSGFv4385lRr"},"source":["## IMPORT RELEVANT LIBRARIES"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZKH8turL5fB9","executionInfo":{"status":"ok","timestamp":1605975349332,"user_tz":-480,"elapsed":3490,"user":{"displayName":"Nicholas Neo","photoUrl":"","userId":"02359484661843204503"}},"outputId":"b48ee314-3f0a-431e-b8b3-c65a600177ff"},"source":["# import the nltk library to do POS tagging\n","import nltk\n","from nltk.tokenize import word_tokenize\n","from nltk.corpus import stopwords\n","nltk.download('averaged_perceptron_tagger')\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","stop_words = set(stopwords.words('english')) "],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"MYai7kktGFsW"},"source":["# import other libraries\n","import numpy as np\n","import string\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.keras.models import load_model\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","import random"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"P3Dun89D6CYZ"},"source":["## DECLARING CONSTANTS"]},{"cell_type":"code","metadata":{"id":"QUn3Q8Jl55Ji"},"source":["# FIXED CONSTANTS\n","WORD_LIMIT = 500000\n","LENGTH = 50 + 1  # input seed (50) + predicted output (1)\n","NO_OF_OUTPUT_WORDS = 30  # number of text predicted/generated\n","\n","# file paths of the dataset\n","DATA = [\"SgCorpus\"]\n","COLAB_FILEPATH = './drive/My Drive/next-sentence-predictor/finalData/'\n","WEIGHTS_DIR = './drive/My Drive/next-sentence-predictor/saved_weights/'"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"H4UMuUaY6NMJ"},"source":["## LOAD & PREPROCESS THE DATA"]},{"cell_type":"code","metadata":{"id":"hP6GWyt26H0_"},"source":["# load doc into memory\n","def load_doc(filename):\n","    # open the file as read only\n","    file = open(filename, 'r')\n","    # read all text\n","    text = file.read()\n","    text = text.lower()\n","    # close the file\n","    file.close()\n","    return text\n","\n","# FUNCTION TO FURTHER CLEAN THE SCRAPED DATASET\n","def clean_txt(doc):\n","    tokens = doc.split()\n","    table = str.maketrans('','',string.punctuation)\n","    tokens = [w.translate(table) for w in tokens]\n","    tokens = [word for word in tokens if word.isalpha()]\n","    tokens = [word.lower() for word in tokens]\n","    return tokens"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QJSQegY96Son"},"source":["# DECLARE THE FINAL FILEPATH TO LOAD THE DATA\n","filename = COLAB_FILEPATH + DATA[0] + '.txt'\n","# CALL FUNCTION TO LOAD RAW DATA\n","data = load_doc(filename)\n","\n","# PASS THE RAW DATA INTO THE FUNCTION\n","tokens = clean_txt(data)\n","\n","# GET WORD SEQUENCES\n","lines = list()\n","for i in range(LENGTH, len(tokens),2): # skip a word for each word sequences\n","    seq = tokens[i-LENGTH:i]\n","    line = ' '.join(seq)\n","    lines.append(line)\n","    # resource constraint (colab RAM), \n","    # take only the first (number of words =  WORD_LIMIT) it encounters\n","    if i > WORD_LIMIT:\n","        break\n","\n","# TOKENIZE TEXT SEQUENCE\n","tokenizer = Tokenizer()\n","tokenizer.fit_on_texts(lines)\n","sequences = tokenizer.texts_to_sequences(lines)\n","\n","# convert to numpy array\n","sequences = np.array(sequences)\n","\n","# assign X and y\n","X, y = sequences[:,:-1], sequences[:,-1]\n","\n","# SIZE OF THE VOCAB\n","vocab_size = len(tokenizer.word_index) + 1\n","\n","# one-hot the output y\n","y = to_categorical(y, num_classes=vocab_size, dtype='int8')\n","\n","# GET THE SEQUENCE LENGTH\n","seq_length = X.shape[1]\n","\n","# OUTPUT DIMENSION OF THE EMBEDDING LAYER\n","EM_OUTPUT_LENGTH = 50"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IxFCTdOX6x6Z"},"source":["## HELPER FUNCTIONS TO LOAD & GENERATE TEXT\n","-> Load the pre-trained H5 files   \n","-> Generate text sequence   \n","-> Show generated text  "]},{"cell_type":"code","metadata":{"id":"jxxwpJG47D2Y"},"source":["# LOAD PRE-TRAINED H5 FILES\n","# Write a function to load different variant of the pre-trained model\n","def load_pretrain_model(model_name,batch_size,epochs, optimizer_type):\n","    filepath = f\"{WEIGHTS_DIR}{model_name}-{epochs}_epoch-{batch_size}_batch_size-{optimizer_type}.h5\"\n","    # debug\n","    #print(filepath)\n","\n","    # load the model\n","    model = load_model(filepath)\n","    #model.summary()\n","\n","    return model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"X_jg4If77clf"},"source":["# GENERATE TEXT SEQUENCES\n","def generate_text_seq(model, tokenizer, text_seq_length, seed_text, n_words):\n","  text = list()\n","\n","  for _ in range(n_words):\n","    # [0] - 0th dimension of the array which contains the encoded text (number tag)\n","    encoded = tokenizer.texts_to_sequences([seed_text])[0] \n","    encoded = pad_sequences([encoded], maxlen = text_seq_length, truncating='pre')\n","\n","    # predict the probability of each word\n","    #y_pred = model.predict_classes(encoded)\n","    y_pred = np.argmax(model.predict(encoded), axis=-1)\n","\n","    predicted_word = ''\n","    for word, index in tokenizer.word_index.items():\n","      if index == y_pred:\n","        predicted_word = word\n","        break\n","    seed_text = seed_text + ' ' + predicted_word\n","    text.append(predicted_word)\n","\n","  return ' '.join(text)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jYRl8ZfU7gw4"},"source":["# SHOW GENERATED TEXT\n","def show_generated_text(model, tokenizer, seq_length, no_of_output_words, own_text=\"eating and sleeping\",seed=12345):\n","    # SEED TEXT GOTTEN FROM THE CORPUS\n","    seed_text_from_corpus = lines[seed]\n","    #print(f\"seed_text_from_corpus: {seed_text_from_corpus}\")\n","    #print()\n","\n","    # OUR OWN SEED TEXT\n","    seed_text_own = own_text\n","    #print(seed_text_own)\n","\n","    # generate next N words after seed text\n","    # seed text from corpus\n","    #var_corpus = generate_text_seq(model, tokenizer, seq_length, seed_text_from_corpus, no_of_output_words)\n","    #print(\"Text generated from corpus seed text:\")\n","    #print(f\"{seed_text_from_corpus} -> {var_corpus}\")\n","\n","    #print()\n","\n","    # seed text from own text\n","    var_own = generate_text_seq(model, tokenizer, seq_length, seed_text_own, no_of_output_words)\n","    print(\"Text generated from own seed text:\")\n","    print(f\"{seed_text_own} {var_own}\")\n","    full_sentence = seed_text_own + \" \" + var_own\n","    \n","    return full_sentence"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"V0E2x-Qr-BBs"},"source":["## HELPER FUNCTIONS FOR POS TAGGING\n","-> Tagging the generated sentence  \n","-> Insertion of singlish words into the generated sentence"]},{"cell_type":"code","metadata":{"id":"6QWLy_me-6oR"},"source":["# Tagging the generated sentence\n","def sentenceTagging(sentence):\n","    #Initialize tagged list\n","    tagged_list = list()\n","\n","    # Tokenize words\n","    wordsList = word_tokenize(sentence)\n","    # removing stop words from wordList \n","    wordsList = [w for w in wordsList]  \n","  \n","    # Using a Tagger. Which is part-of-speech  \n","    # tagger or POS-tagger.  \n","    tagged = nltk.pos_tag(wordsList)\n","    for item in tagged:\n","        tagged_list.append(item)\n","    return tagged_list"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"s_zjhHHT_6f4"},"source":["# Insertion of singlish words into the generated sentence\n","def singlify(tagged_sentence, singlish_adj_list):\n","  singlish_sentence = \"\"\n","  for pair in tagged_sentence:\n","    word = pair[0]\n","    type_of_word = pair[1]\n","    # Replace words if they are the following (more Singlish way of representing in text)\n","    if word == 'zero':\n","      word = 'jilo'\n","    elif word == 'copy':\n","      word = 'kope'\n","    elif word == 'already':\n","      word = 'orredy'\n","    elif word == 'very':\n","      word = 'very the'\n","    elif word == 'vomit':\n","      word = 'womit'\n","    elif word == 'better':\n","      word = 'more better'\n","    \n","    # Looking for adjectives in the tagged sentence\n","    if type_of_word == 'JJ':\n","      singlish_word = random.choice(singlish_adj_list)\n","      \n","      # if following word is adjective, insert the Singlish word after the next word\n","      singlish_sentence = singlish_sentence + \" \" + word + \" \" + singlish_word\n","    else:\n","      # else just insert the next word\n","      singlish_sentence = singlish_sentence + \" \" + word\n","\n","  # remove the first spacing\n","  singlish_sentence_ = singlish_sentence[1:]\n","  \n","  # Return sentence\n","  return singlish_sentence_"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"W9gDnUO3Cgbc"},"source":["# a composite function that each of the generated text could call\n","def insert_singlish_word(raw_sentence, singlish_adj_list):\n","    tagged_sentence = sentenceTagging(raw_sentence)\n","    singlish_sentence = singlify(tagged_sentence, singlish_adj_list)\n","\n","    return singlish_sentence"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hSFNSf4EA13A"},"source":["## LIST OF SINGLISH ADJECTIVE LIST"]},{"cell_type":"code","metadata":{"id":"4BlL9hPZPbYd"},"source":["SINGLISH_ADJ_LIST = [\"lah\",\"lorh\",\"leh\",\"sia\",\"mah\",\"meh\",\"kenna\",\"liao\",\"siao\"]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZVoyUg2s72a3"},"source":["## LOAD PRE-TRAINED H5 FILE & SHOW GENERATED TEXT\n","**Selected models**  \n","1. Stacked LSTM + Adam + Batch Size 128 + 20 epochs  \n","2. Stacked LSTM + Adam + Batch Size 128 + 100 epochs  \n","3. Stacked LSTM + Adam + Batch Size 64 + 100 epochs   \n","4. Stacked GRU + Adam + Batch Size 64 + 100 epochs\n","\n","**Selected text starter**  \n","1. here i am testing my nlg project..  \n","2. never did i have...  \n","3. someday i will..."]},{"cell_type":"markdown","metadata":{"id":"6cB9zoax9QtP"},"source":["### FIXED CONSTANTS USED"]},{"cell_type":"code","metadata":{"id":"Kc5Akmk69gO5"},"source":["# initialise number of epochs to train the model\n","EPOCHS = 100\n","SEED = 1234"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vjk-uY5S9kXA"},"source":["## TEXT STARTER 1: here i am testing my nlg project..."]},{"cell_type":"code","metadata":{"id":"R7ptpGre7_tG"},"source":["# initialise the starting word/phrase of our own text and seed count\n","OWN_TEXT_STARTER = \"here i am testing my nlg project\""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uoYf-e-7NRV9"},"source":["#### Stacked LSTM + Adam + 20 Epochs + Batch Size 128"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"avpegTQg8WZZ","executionInfo":{"status":"ok","timestamp":1605970157780,"user_tz":-480,"elapsed":2908,"user":{"displayName":"Nicholas Neo","photoUrl":"","userId":"02359484661843204503"}},"outputId":"004dfdec-9981-4fd6-a7dc-f78bfef225f2"},"source":["# LSTM + Adam + 20 Epochs + Batch Size 128\n","# load pretrained model\n","model = load_pretrain_model(model_name=\"2LAYER_LSTM\", batch_size=128, epochs=20, optimizer_type=\"Adam\")\n","\n","# generate the sentences given the text starter\n","full_sentence = show_generated_text(model, tokenizer, seq_length, NO_OF_OUTPUT_WORDS, own_text=OWN_TEXT_STARTER, seed=SEED)\n","\n","# generate the Singlish sentence \n","singlish_sentence = insert_singlish_word(full_sentence, SINGLISH_ADJ_LIST)\n","print(\"\\nSinglish sentence:\")\n","print(singlish_sentence)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Text generated from own seed text:\n","here i am testing my nlg project and i am a crush on my og and i am not really nice i am not a crush on my og and i am not really nice i am\n","\n","Singlish sentence:\n","here i lorh am testing my nlg project and i am a crush on my og and i am not really nice siao i am not a crush on my og and i am not really nice meh i am\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"WG48qlKtNdBR"},"source":["#### Stacked LSTM + Adam + 100 Epochs + Batch Size 128"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4r0mXJBi983H","executionInfo":{"status":"ok","timestamp":1605970223629,"user_tz":-480,"elapsed":3242,"user":{"displayName":"Nicholas Neo","photoUrl":"","userId":"02359484661843204503"}},"outputId":"daa3e7f2-a4b8-4500-83cf-a473b0bb78f6"},"source":["# LSTM + Adam + 100 Epochs + Batch Size 128\n","# load pretrained model\n","model = load_pretrain_model(model_name=\"2LAYER_LSTM\", batch_size=128, epochs=EPOCHS, optimizer_type=\"Adam\")\n","\n","# generate the sentences given the text starter\n","full_sentence = show_generated_text(model, tokenizer, seq_length, NO_OF_OUTPUT_WORDS, own_text=OWN_TEXT_STARTER, seed=SEED)\n","\n","# generate the Singlish sentence \n","singlish_sentence = insert_singlish_word(full_sentence, SINGLISH_ADJ_LIST)\n","print(\"\\nSinglish sentence:\")\n","print(singlish_sentence)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Text generated from own seed text:\n","here i am testing my nlg project customers subscribe for the latest food news reviewsfacebook daniels food diaryinstagram food videos singapore deli chef recipes off on this new outlet by yong noodle can check out tunglok new\n","\n","Singlish sentence:\n","here i lah am testing my nlg project customers subscribe for the latest food news reviewsfacebook daniels food diaryinstagram food videos singapore deli liao chef recipes off on this new siao outlet by yong meh noodle can check out tunglok new sia\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"0_lCQ--sOn1k"},"source":["#### Stacked LSTM + Adam + 100 epochs + Batch Size 64 "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iIeZWZe_OFY5","executionInfo":{"status":"ok","timestamp":1605970233229,"user_tz":-480,"elapsed":3415,"user":{"displayName":"Nicholas Neo","photoUrl":"","userId":"02359484661843204503"}},"outputId":"9aa90335-a0eb-47cf-d325-1e61019464d0"},"source":["# LSTM + Adam + 100 Epochs + Batch Size 64\n","# load pretrained model\n","model = load_pretrain_model(model_name=\"2LAYER_LSTM\", batch_size=64, epochs=EPOCHS, optimizer_type=\"Adam\")\n","\n","# generate the sentences given the text starter\n","full_sentence = show_generated_text(model, tokenizer, seq_length, NO_OF_OUTPUT_WORDS, own_text=OWN_TEXT_STARTER, seed=SEED)\n","\n","# generate the Singlish sentence \n","singlish_sentence = insert_singlish_word(full_sentence, SINGLISH_ADJ_LIST)\n","print(\"\\nSinglish sentence:\")\n","print(singlish_sentence)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Text generated from own seed text:\n","here i am testing my nlg project back craze spend loo speakers blogger was being that delivering job on drama masks for heavy session at the past courses and hasi is supporting me because the rest is\n","\n","Singlish sentence:\n","here i kenna am testing my nlg project back craze kenna spend loo speakers blogger was being that delivering job on drama masks for heavy leh session at the past liao courses and hasi is supporting me because the rest is\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"cArVNLE8Qsc_"},"source":["#### Stacked GRU + Adam + 100 epochs + Batch Size 64 "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sx5rRX3vOu9u","executionInfo":{"status":"ok","timestamp":1605970263031,"user_tz":-480,"elapsed":3069,"user":{"displayName":"Nicholas Neo","photoUrl":"","userId":"02359484661843204503"}},"outputId":"78a3437b-eb09-48e4-b517-54281922df5a"},"source":["# LSTM + Adam + 100 Epochs + Batch Size 64\n","# load pretrained model\n","model = load_pretrain_model(model_name=\"2LAYER_GRU\", batch_size=64, epochs=EPOCHS, optimizer_type=\"Adam\")\n","\n","# generate the sentences given the text starter\n","full_sentence = show_generated_text(model, tokenizer, seq_length, NO_OF_OUTPUT_WORDS, own_text=OWN_TEXT_STARTER, seed=SEED)\n","\n","# generate the Singlish sentence \n","singlish_sentence = insert_singlish_word(full_sentence, SINGLISH_ADJ_LIST)\n","print(\"\\nSinglish sentence:\")\n","print(singlish_sentence)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Text generated from own seed text:\n","here i am testing my nlg project ceo kate forever awesome ceo air on kandie flats ceo closing forever playing foreign workers wearing todays projects lets actually enjoying playing roasted resort date ceo lee roasted dumplings man\n","\n","Singlish sentence:\n","here i sia am testing my nlg project ceo kate forever awesome mah ceo air on kandie flats ceo closing forever playing foreign liao workers wearing todays projects lets actually enjoying playing roasted resort date ceo lee roasted dumplings man\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"w9rHzmI8S1JM"},"source":["## TEXT STARTER 2: never did i have..."]},{"cell_type":"code","metadata":{"id":"Vhuk0AKeQ5EV"},"source":["# initialise the starting word/phrase of our own text and seed count\n","OWN_TEXT_STARTER = \"never did i have\""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yFnNR1NPS-r7"},"source":["#### Stacked LSTM + Adam + 20 Epochs + Batch Size 128"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4PysB9hhS8vF","executionInfo":{"status":"ok","timestamp":1605975492514,"user_tz":-480,"elapsed":17650,"user":{"displayName":"Nicholas Neo","photoUrl":"","userId":"02359484661843204503"}},"outputId":"3582e922-bfd3-4abb-f507-40978c05fd17"},"source":["# LSTM + Adam + 20 Epochs + Batch Size 128\n","# load pretrained model\n","model = load_pretrain_model(model_name=\"2LAYER_LSTM\", batch_size=128, epochs=20, optimizer_type=\"Adam\")\n","\n","# generate the sentences given the text starter\n","full_sentence = show_generated_text(model, tokenizer, seq_length, NO_OF_OUTPUT_WORDS, own_text=OWN_TEXT_STARTER, seed=SEED)\n","\n","# generate the Singlish sentence \n","singlish_sentence = insert_singlish_word(full_sentence, SINGLISH_ADJ_LIST)\n","print(\"\\nSinglish sentence:\")\n","print(singlish_sentence)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Text generated from own seed text:\n","never did i have been a crush on my og is a lot of the best for jubilant you be a good to be a good time to be a good thing in the\n","\n","Singlish sentence:\n","never did i have been a crush on my og is a lot of the best for jubilant you be a good liao to be a good sia time to be a good leh thing in the\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"2NTFyCHvS_JD"},"source":["#### Stacked LSTM + Adam + 100 Epochs + Batch Size 128"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xm2ROK4ZS_P0","executionInfo":{"status":"ok","timestamp":1605975495546,"user_tz":-480,"elapsed":9113,"user":{"displayName":"Nicholas Neo","photoUrl":"","userId":"02359484661843204503"}},"outputId":"83e16f14-3fea-4195-ff68-ad88442828e7"},"source":["# LSTM + Adam + 100 Epochs + Batch Size 128\n","# load pretrained model\n","model = load_pretrain_model(model_name=\"2LAYER_LSTM\", batch_size=128, epochs=EPOCHS, optimizer_type=\"Adam\")\n","\n","# generate the sentences given the text starter\n","full_sentence = show_generated_text(model, tokenizer, seq_length, NO_OF_OUTPUT_WORDS, own_text=OWN_TEXT_STARTER, seed=SEED)\n","\n","# generate the Singlish sentence \n","singlish_sentence = insert_singlish_word(full_sentence, SINGLISH_ADJ_LIST)\n","print(\"\\nSinglish sentence:\")\n","print(singlish_sentence)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Text generated from own seed text:\n","never did i have deserve favours was the schools is chosen thesmartlocalcom away the girls out some ppl hes super terrible bottle of the cable system from stage of aug this was an hat\n","\n","Singlish sentence:\n","never did i have deserve favours was the schools is chosen thesmartlocalcom away the girls out some ppl hes super terrible liao bottle of the cable system from stage of aug this was an hat\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"QAXv0V1CS_WF"},"source":["#### Stacked LSTM + Adam + 100 epochs + Batch Size 64"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YR2Rxw2NS_bY","executionInfo":{"status":"ok","timestamp":1605975499338,"user_tz":-480,"elapsed":6064,"user":{"displayName":"Nicholas Neo","photoUrl":"","userId":"02359484661843204503"}},"outputId":"b9e1ebaf-8c43-4cab-9c1d-b8e067465008"},"source":["# LSTM + Adam + 100 Epochs + Batch Size 64\n","# load pretrained model\n","model = load_pretrain_model(model_name=\"2LAYER_LSTM\", batch_size=64, epochs=EPOCHS, optimizer_type=\"Adam\")\n","\n","# generate the sentences given the text starter\n","full_sentence = show_generated_text(model, tokenizer, seq_length, NO_OF_OUTPUT_WORDS, own_text=OWN_TEXT_STARTER, seed=SEED)\n","\n","# generate the Singlish sentence \n","singlish_sentence = insert_singlish_word(full_sentence, SINGLISH_ADJ_LIST)\n","print(\"\\nSinglish sentence:\")\n","print(singlish_sentence)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Text generated from own seed text:\n","never did i have crossed his results at tedx n was my initials community area there meet fighting pls want a wrong impression i forgot with stuff from music dollarsandsense family last list of\n","\n","Singlish sentence:\n","never did i have crossed his results at tedx sia n was my initials community area there meet fighting pls want a wrong meh impression i forgot with stuff from music dollarsandsense family last mah list of\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"URIVXRW6S_gW"},"source":["#### Stacked GRU + Adam + 100 epochs + Batch Size 64"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8Eucl6Z7S_lt","executionInfo":{"status":"ok","timestamp":1605975502772,"user_tz":-480,"elapsed":7447,"user":{"displayName":"Nicholas Neo","photoUrl":"","userId":"02359484661843204503"}},"outputId":"a5031d0e-abd9-432a-d7b6-781f3d9d8fd7"},"source":["# LSTM + Adam + 100 Epochs + Batch Size 64\n","# load pretrained model\n","model = load_pretrain_model(model_name=\"2LAYER_GRU\", batch_size=64, epochs=EPOCHS, optimizer_type=\"Adam\")\n","\n","# generate the sentences given the text starter\n","full_sentence = show_generated_text(model, tokenizer, seq_length, NO_OF_OUTPUT_WORDS, own_text=OWN_TEXT_STARTER, seed=SEED)\n","\n","# generate the Singlish sentence \n","singlish_sentence = insert_singlish_word(full_sentence, SINGLISH_ADJ_LIST)\n","print(\"\\nSinglish sentence:\")\n","print(singlish_sentence)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Text generated from own seed text:\n","never did i have hit forever on becoming awesome earlier ceo water a wealth ceo ceo date ceo rewards roasted diary new view liang coupons live using the high food highlights the worlds area\n","\n","Singlish sentence:\n","never did i have hit forever on becoming awesome leh earlier ceo siao water a wealth ceo ceo date ceo rewards roasted diary liao new leh view liang mah coupons live using the high leh food highlights the worlds area\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"pfBUUTS_Ui_5"},"source":["## TEXT STARTER 3: someday i will..."]},{"cell_type":"code","metadata":{"id":"-FD1kcyYTf2i"},"source":["# initialise the starting word/phrase of our own text and seed count\n","OWN_TEXT_STARTER = \"someday i will\""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3VJ602NSUu4D"},"source":["#### Stacked LSTM + Adam + 20 Epochs + Batch Size 128"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mqjAwJm6Ur7L","executionInfo":{"status":"ok","timestamp":1605970954183,"user_tz":-480,"elapsed":3059,"user":{"displayName":"Nicholas Neo","photoUrl":"","userId":"02359484661843204503"}},"outputId":"36fd115d-cc98-417e-afe4-c4187e3b7c86"},"source":["# LSTM + Adam + 20 Epochs + Batch Size 128\n","# load pretrained model\n","model = load_pretrain_model(model_name=\"2LAYER_LSTM\", batch_size=128, epochs=20, optimizer_type=\"Adam\")\n","\n","# generate the sentences given the text starter\n","full_sentence = show_generated_text(model, tokenizer, seq_length, NO_OF_OUTPUT_WORDS, own_text=OWN_TEXT_STARTER, seed=SEED)\n","\n","# generate the Singlish sentence \n","singlish_sentence = insert_singlish_word(full_sentence, SINGLISH_ADJ_LIST)\n","print(\"\\nSinglish sentence:\")\n","print(singlish_sentence)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Text generated from own seed text:\n","someday i will be shocked on the past years and i am been a good in the end of the best for yur in the end of the end of the end of\n","\n","Singlish sentence:\n","someday i will be shocked on the past liao years and i am been a good lah in the end of the best for yur in the end of the end of the end of\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"HPL7o3WxUwdr"},"source":["#### Stacked LSTM + Adam + 100 Epochs + Batch Size 128"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kQcWEWpSUwlG","executionInfo":{"status":"ok","timestamp":1605970964406,"user_tz":-480,"elapsed":3059,"user":{"displayName":"Nicholas Neo","photoUrl":"","userId":"02359484661843204503"}},"outputId":"9d6132fd-e861-4e04-aca8-c9b299151f7b"},"source":["# LSTM + Adam + 100 Epochs + Batch Size 128\n","# load pretrained model\n","model = load_pretrain_model(model_name=\"2LAYER_LSTM\", batch_size=128, epochs=EPOCHS, optimizer_type=\"Adam\")\n","\n","# generate the sentences given the text starter\n","full_sentence = show_generated_text(model, tokenizer, seq_length, NO_OF_OUTPUT_WORDS, own_text=OWN_TEXT_STARTER, seed=SEED)\n","\n","# generate the Singlish sentence \n","singlish_sentence = insert_singlish_word(full_sentence, SINGLISH_ADJ_LIST)\n","print(\"\\nSinglish sentence:\")\n","print(singlish_sentence)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Text generated from own seed text:\n","someday i will be yesterday for frugal type channel with my family destination careerremember of bubble tea at citylink mall danielfooddiarycomwhere to say about the details to become said to get the boyfriend\n","\n","Singlish sentence:\n","someday i will be yesterday for frugal lorh type channel with my family destination careerremember of bubble sia tea at citylink mall danielfooddiarycomwhere to say about the details to become said to get the boyfriend\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"W9yc8FYeUwrr"},"source":["#### Stacked LSTM + Adam + 100 epochs + Batch Size 64"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DiqzEoCCUwx1","executionInfo":{"status":"ok","timestamp":1605970975649,"user_tz":-480,"elapsed":2993,"user":{"displayName":"Nicholas Neo","photoUrl":"","userId":"02359484661843204503"}},"outputId":"a38d9236-8584-43e5-cead-a76d9ebc488f"},"source":["# LSTM + Adam + 100 Epochs + Batch Size 64\n","# load pretrained model\n","model = load_pretrain_model(model_name=\"2LAYER_LSTM\", batch_size=64, epochs=EPOCHS, optimizer_type=\"Adam\")\n","\n","# generate the sentences given the text starter\n","full_sentence = show_generated_text(model, tokenizer, seq_length, NO_OF_OUTPUT_WORDS, own_text=OWN_TEXT_STARTER, seed=SEED)\n","\n","# generate the Singlish sentence \n","singlish_sentence = insert_singlish_word(full_sentence, SINGLISH_ADJ_LIST)\n","print(\"\\nSinglish sentence:\")\n","print(singlish_sentence)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Text generated from own seed text:\n","someday i will get to pull down and honestly i am sure aj simply dance classics next other sp wins when it was such an amazing picture keep actually peace with of bento\n","\n","Singlish sentence:\n","someday i will get to pull down and honestly i siao am sure sia aj simply dance classics next other lorh sp wins when it was such siao an amazing meh picture keep actually peace with of bento\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"jF6AZFJ0Uw36"},"source":["#### Stacked GRU + Adam + 100 epochs + Batch Size 64"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JMyKSC37Uw_b","executionInfo":{"status":"ok","timestamp":1605970991025,"user_tz":-480,"elapsed":3009,"user":{"displayName":"Nicholas Neo","photoUrl":"","userId":"02359484661843204503"}},"outputId":"8d05a660-e6cc-4ce4-ed79-cd69d9cadcb5"},"source":["# LSTM + Adam + 100 Epochs + Batch Size 64\n","# load pretrained model\n","model = load_pretrain_model(model_name=\"2LAYER_GRU\", batch_size=64, epochs=EPOCHS, optimizer_type=\"Adam\")\n","\n","# generate the sentences given the text starter\n","full_sentence = show_generated_text(model, tokenizer, seq_length, NO_OF_OUTPUT_WORDS, own_text=OWN_TEXT_STARTER, seed=SEED)\n","\n","# generate the Singlish sentence \n","singlish_sentence = insert_singlish_word(full_sentence, SINGLISH_ADJ_LIST)\n","print(\"\\nSinglish sentence:\")\n","print(singlish_sentence)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Text generated from own seed text:\n","someday i will wanted to work on kim gold ceo bright koh lee wine sheng sea coffeehouse ceo daily bellywellyjelly discount at todays date in kandie chen steamboat arrives code koh pink roasted\n","\n","Singlish sentence:\n","someday i will wanted to work on kim gold meh ceo bright lah koh lee wine sheng sea coffeehouse ceo lorh daily leh bellywellyjelly discount at todays siao date in kandie chen steamboat arrives code liao koh pink roasted\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"erOE67OVVIuj"},"source":[""],"execution_count":null,"outputs":[]}]}